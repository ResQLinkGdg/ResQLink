package com.example.resqlink.rag.generation

import android.content.Context
import android.util.Log
import com.example.resqlink.BuildConfig
import com.google.ai.client.generativeai.GenerativeModel
import com.google.ai.client.generativeai.type.content
import com.google.ai.client.generativeai.type.generationConfig
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext

class InferenceModel(private val context: Context) {

    // ğŸŸ¢ Gemini ëª¨ë¸ ì„¤ì •
    // flash ëª¨ë¸ì´ ë¹ ë¥´ê³  ì €ë ´í•˜ë©° RAG ë‹µë³€ìš©ìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.
    private val generativeModel = GenerativeModel(
        modelName = "gemini-1.5-flash",
        apiKey = BuildConfig.GEMINI_API_KEY, // build.gradleì—ì„œ ì„¤ì •í•œ í‚¤ ì‚¬ìš©
        generationConfig = generationConfig {
            temperature = 0.5f // ë‹µë³€ì˜ ì°½ì˜ì„± ì¡°ì ˆ (0.0 ~ 1.0)
            topK = 40
            topP = 0.95f
            maxOutputTokens = 1024
        }
    )

    // ë” ì´ìƒ ë¬´ê±°ìš´ ëª¨ë¸ ì´ˆê¸°í™”(íŒŒì¼ ë³µì‚¬ ë“±)ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    suspend fun initialize() {
        Log.d("InferenceModel", "Gemini API Client Ready")
    }

    suspend fun generateResponse(prompt: String): String? {
        return withContext(Dispatchers.IO) {
            try {
                Log.d("InferenceModel", "Requesting Gemini API...")

                // API í˜¸ì¶œ
                val response = generativeModel.generateContent(prompt)

                val answer = response.text
                Log.d("InferenceModel", "Gemini Response received: ${answer?.take(50)}...")

                answer
            } catch (e: Exception) {
                Log.e("InferenceModel", "Gemini API Error: ${e.message}", e)
                "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”)"
            }
        }
    }

    fun close() {
        // API í´ë¼ì´ì–¸íŠ¸ëŠ” íŠ¹ë³„í•œ í•´ì œ ì‘ì—…ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    }
}

//package com.example.resqlink.rag.generation
//
//import android.content.Context
//import android.util.Log
//import com.google.mediapipe.tasks.genai.llminference.LlmInference
//import kotlinx.coroutines.Dispatchers
//import kotlinx.coroutines.withContext
//import java.io.File
//
//class InferenceModel(private val context: Context) {
//
//    private var llmInference: LlmInference? = null
//    private val modelFileName = "gemma3-1B-it-int4.tflite"
//
//    suspend fun initialize() = withContext(Dispatchers.IO) {
//        if (llmInference != null) return@withContext
//
//        try {
//            Log.d("InferenceModel", "ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
//
//            val modelFile = File(context.filesDir, modelFileName)
//            if (!modelFile.exists()) {
//                Log.d("InferenceModel", "ëª¨ë¸ íŒŒì¼ ë³µì‚¬ ì¤‘...")
//                context.assets.open(modelFileName).use { inputStream ->
//                    modelFile.outputStream().use { outputStream ->
//                        inputStream.copyTo(outputStream)
//                    }
//                }
//            }
//
//            val options = LlmInference.LlmInferenceOptions.builder()
//                .setModelPath(modelFile.absolutePath)
//                .setMaxTokens(1024) // ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
//                .setTemperature(0.7f)
//                .setTopK(40)
//                .build()
//
//            llmInference = LlmInference.createFromOptions(context, options)
//            Log.d("InferenceModel", "ì˜¨ë””ë°”ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
//
//        } catch (e: Exception) {
//            Log.e("InferenceModel", "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨", e)
//            // ì—ëŸ¬ê°€ ë‚˜ë„ ì•±ì´ ì£½ì§€ ì•Šë„ë¡ ì˜ˆì™¸ ì²˜ë¦¬
//        }
//    }
//
//    suspend fun generateResponse(prompt: String): String = withContext(Dispatchers.IO) {
//        try {
//            if (llmInference == null) {
//                return@withContext "ëª¨ë¸ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."
//            }
//
//            // Gemma 3 í”„ë¡¬í”„íŠ¸ í¬ë§·
//            val formattedPrompt = """
//                <start_of_turn>user
//                $prompt<end_of_turn>
//                <start_of_turn>model
//            """.trimIndent()
//
//            // ì¶”ë¡  ì‹¤í–‰
//            llmInference?.generateResponse(formattedPrompt) ?: "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"
//        } catch (e: Exception) {
//            Log.e("InferenceModel", "ì¶”ë¡  ì¤‘ ì—ëŸ¬ ë°œìƒ", e)
//            "ì—ëŸ¬ ë°œìƒ: ${e.message}"
//        }
//    }
//
//    // ë©”ëª¨ë¦¬ í•´ì œê°€ í•„ìš”í•  ë•Œ í˜¸ì¶œ
//    fun close() {
//        llmInference = null
//    }
//}